% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compute_performance_metrics.R
\name{compute_performance_metrics}
\alias{compute_performance_metrics}
\title{Compute Performance Metrics for Binary Classification}
\usage{
compute_performance_metrics(labels, predictions, threshold = 0.5)
}
\arguments{
\item{labels}{Binary outcome vector (0/1)}

\item{predictions}{Predicted probabilities (0-1)}

\item{threshold}{Classification threshold (default = 0.5)}
}
\value{
A list containing:
\itemize{
\item \code{auc}: Area Under ROC Curve
\item \code{f1}: F1-Score
\item \code{recall}: Recall (Sensitivity)
\item \code{precision}: Precision (PPV)
\item \code{specificity}: Specificity
\item \code{accuracy}: Overall accuracy
\item \code{brier}: Brier score
\item \code{ece}: Expected Calibration Error
}
}
\description{
Calculates discrimination and calibration metrics for binary classification predictions.
}
